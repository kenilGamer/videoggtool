# LLM Provider Configuration
# Choose: 'ollama' (FREE - local), 'openai', or 'anthropic'
LLM_PROVIDER=ollama

# Ollama Configuration (FREE - runs locally, no API key needed)
# 1. Install Ollama from: https://ollama.ai
# 2. Run: ollama pull llama3.2
# 3. Set provider to 'ollama' (default model: llama3.2)
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (requires API key and billing)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your-openai-api-key-here

# Anthropic Configuration (requires API key)
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Optional LLM Settings
# LLM_MODEL=llama3.2
# LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS=4000

# FFmpeg Configuration (optional)
# FFMPEG_PATH=ffmpeg

# Output Directory (optional)
# OUTPUT_DIR=./output

# Server Configuration (optional)
# PORT=3000
